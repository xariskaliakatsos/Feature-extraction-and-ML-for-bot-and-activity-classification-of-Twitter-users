{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "453ddae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy # tweepy module to interact with Twitter\n",
    "import pandas as pd # Pandas library to create dataframes\n",
    "from tweepy import OAuthHandler # Used for authentication\n",
    "from tweepy import Cursor # Used to perform pagination\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4247a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#using my credentials with academic access.\n",
    "\n",
    "\n",
    "cons_key = 'XXXX'\n",
    "cons_secret = 'XXXXX'\n",
    "acc_token = 'XXXXX'\n",
    "acc_secret = 'XXXXX'\n",
    "bearer_token = 'XXXXXX'\n",
    "\n",
    "\n",
    "# (1). Athentication Function\n",
    "def get_twitter_auth(): #the authentification to Twitter\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        consumer_key = cons_key\n",
    "        consumer_secret = cons_secret\n",
    "        access_token = acc_token\n",
    "        access_secret = acc_secret\n",
    "        \n",
    "    except KeyError:\n",
    "        sys.stderr.write(\"Twitter Environment Variable not Set\\n\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    \n",
    "    return auth #Function to access the authentication API\n",
    "def get_twitter_client():   #the client to access the authentification API\n",
    "\n",
    " \n",
    "    auth = get_twitter_auth()\n",
    "    client = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    return client\n",
    "#Function creating final dataframe\n",
    "def get_tweets_from_user(ids, page_limit=2, count_tweet=200):\n",
    "\n",
    "    #ids: the twitter username of a user (company, etc.)\n",
    "    #page_limit: the total number of pages (max=16)\n",
    "    #count_tweet: maximum number to be retrieved from a page\n",
    "        \n",
    "\n",
    "    client = get_twitter_client()\n",
    "    \n",
    "    all_tweets = []\n",
    "    \n",
    "    for page in Cursor(client.user_timeline, \n",
    "                user_id=ids, \n",
    "                count=count_tweet,tweet_mode='extended').pages(page_limit):\n",
    "        for tweet in page:\n",
    "            parsed_tweet = {}\n",
    "            parsed_tweet['source'] = tweet.source #get the source of the tweet (e.g. iphone, android)\n",
    "            parsed_tweet['all_info'] = tweet._json #get all the rest of the data\n",
    "                \n",
    "            all_tweets.append(parsed_tweet)\n",
    "            if len(all_tweets) >200: #Limit the tweets to 200\n",
    "                break\n",
    "    # Create dataframe \n",
    "    df = pd.DataFrame(all_tweets)\n",
    "    # Revome duplicates if there are any\n",
    "    df2 = pd.json_normalize(df[\"all_info\"]) #Unpack the data in the all_info column\n",
    "    df2 = df2[[\"created_at\", \"id_str\", 'full_text', \"in_reply_to_user_id_str\", \"retweet_count\",  \"favorite_count\", \"favorited\", \"retweeted\",  \"entities.hashtags\", 'entities.symbols', 'entities.user_mentions', 'entities.urls', 'user.id', 'user.id_str','retweeted_status.created_at', 'retweeted_status.id', 'retweeted_status.id_str']]\n",
    "    df3 = pd.concat([df['source'],df2],axis=1) #concat all data in a final dataframe\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "61ac524b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████                                                                  | 485/3392 [09:40<1:02:10,  1.28s/it]Rate limit reached. Sleeping for: 239\n",
      " 29%|██████████████████████▌                                                        | 969/3392 [23:51<40:43,  1.01s/it]Rate limit reached. Sleeping for: 290\n",
      " 43%|█████████████████████████████████▍                                            | 1452/3392 [39:26<43:23,  1.34s/it]Rate limit reached. Sleeping for: 257\n",
      " 58%|████████████████████████████████████████████▉                                 | 1956/3392 [54:42<32:04,  1.34s/it]Rate limit reached. Sleeping for: 241\n",
      " 73%|███████████████████████████████████████████████████████▏                    | 2463/3392 [1:10:12<24:40,  1.59s/it]Rate limit reached. Sleeping for: 213\n",
      " 87%|██████████████████████████████████████████████████████████████████          | 2950/3392 [1:26:01<11:06,  1.51s/it]Rate limit reached. Sleeping for: 166\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 3392/3392 [1:39:36<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "ids = pd.read_csv('gilani-2017.tsv',header=None,delim_whitespace=True) #read data \n",
    "ids.drop_duplicates(subset=[0],inplace=True,ignore_index=True)\n",
    "second_dataset = pd.read_csv('cresci-rtbust-2019.tsv',header=None, delim_whitespace=True)\n",
    "second_dataset.drop_duplicates(subset=[0],inplace=True,ignore_index=True) #drop duplicate ids\n",
    "ids = pd.concat([ids,second_dataset], axis=0,ignore_index=True) # concat all ids in one file\n",
    "column_names =  [\"source\",\"created_at\", \"id_str\", 'text', \"in_reply_to_user_id_str\", \"retweet_count\",  \"favorite_count\", \"favorited\", \"retweeted\",  \"entities.hashtags\", 'entities.symbols', 'entities.user_mentions', 'entities.urls', 'user.id', 'user.id_str','retweeted_status.created_at', 'retweeted_status.id', 'retweeted_status.id_str']\n",
    "df_final = pd.DataFrame(columns = column_names) #create a dataframe to concat all the collected tweets\n",
    "for i in tqdm(range(len(ids[0]))):\n",
    "    try:\n",
    "        tweets = get_tweets_from_user(ids[0][i]) #itterate to get all the tweets\n",
    "    except Exception as e: #to avoid possible errors from missing account and info\n",
    "        #print(e)\n",
    "        continue\n",
    "    df_final = pd.concat([tweets,df_final])\n",
    "    df_final.reset_index(inplace=True, drop=True)\n",
    "df_final.to_csv('Final_Data_v3.csv') #save all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "satellite-iceland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:18<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "#real users\n",
    "#repeat same steps as above, just with different ids this time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "column_names =  [\"source\",\"created_at\", \"id_str\", 'text', \"in_reply_to_user_id_str\", \"retweet_count\",  \"favorite_count\", \"favorited\", \"retweeted\",  \"entities.hashtags\", 'entities.symbols', 'entities.user_mentions', 'entities.urls', 'user.id', 'user.id_str','retweeted_status.created_at', 'retweeted_status.id', 'retweeted_status.id_str']\n",
    "df_final = pd.DataFrame(columns = column_names)\n",
    "user_ids = [234343780, 48008938, 249130209, 1348351605654106118, 86391789, 70340615, 289527193, 1005766052607938560, 62213337, 1495855082734297095, 1976335622, 1066275282653536256]\n",
    "for i in tqdm(range(len(user_ids))):\n",
    "    try:\n",
    "        tweets = get_tweets_from_user(user_ids[i])\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    df_final = pd.concat([tweets,df_final])\n",
    "    df_final.reset_index(inplace=True, drop=True)\n",
    "df_final.to_csv('Real_users.csv') #Save to Real_users.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd2027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and auth again \n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "\n",
    "CONSUMER_KEY = cons_key\n",
    "CONSUMER_SECRET = cons_secret\n",
    "ACCESS_KEY = acc_token\n",
    "ACCESS_SECRET = acc_secret\n",
    "auth = OAuthHandler(CONSUMER_KEY,CONSUMER_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "\n",
    "#search\n",
    "api = tweepy.API(auth)\n",
    "test = api.lookup_users(user_id=user_ids) #take all the required user info using the lookup_users method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab6451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = json.loads(json.dumps(test[0]._json)) #unpack the data\n",
    "df = pd.DataFrame([json_str], columns=json_str.keys()) #create a dataframe\n",
    "for i in range(len(test)-1): \n",
    "    json_str = json.loads(json.dumps(test[i+1]._json))\n",
    "    df1 = pd.DataFrame([json_str], columns=json_str.keys())\n",
    "    df = pd.concat([df1,df]) #concat \n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df.to_csv('user_data.csv') #save to csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
